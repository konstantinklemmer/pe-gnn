{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFUwfs9AABkL"
      },
      "source": [
        "# PE-GNN - Example notebook\n",
        "\n",
        "This notebook includes the *PE-GNN* method proposed in the paper [\" \tPositional Encoder Graph Neural Networks for Geographic Data \"](https://arxiv.org/abs/2111.10144), implemented in `PyTorch`. It is optimized for running on [Google Colab](colab.research.google.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h7pLYtOqb_Q"
      },
      "source": [
        "## Install Dependencies\n",
        "Our experiments rely on *PyG* / *PyTorch Geometric* for the GNN backbones. See here for more details see: https://github.com/pyg-team/pytorch_geometric "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq2RDPuspqu-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Check version of pytorch and cuda -- following assumes PyTorch version 1.10.0 and cuda 11.1\n",
        "! python -c \"import torch; print(torch.__version__)\"\n",
        "! python -c \"import torch; print(torch.version.cuda)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqkCTKBUogiC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# # Install pytorch geometric -- make sure torch and cuda numbers at the end are consistent with your versoin\n",
        "! pip install --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "! pip install --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "! pip install --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "! pip install --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
        "! pip install torch-geometric\n",
        "! pip install geomloss[full]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHbtSo8OVtsI"
      },
      "source": [
        "Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DXp4CclVd9Qs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from torch.utils.data import Sampler\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch_geometric.nn import GCNConv, knn_graph\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from geopy.distance import distance\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import sys\n",
        "import requests\n",
        "from urllib.request import urlretrieve\n",
        "import urllib.request, json \n",
        "import zipfile\n",
        "import subprocess\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from decimal import Decimal, getcontext\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from scipy import sparse\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn import metrics\n",
        "\n",
        "import torch\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import tqdm\n",
        "\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from urllib.request import urlretrieve\n",
        "import urllib.request, json\n",
        "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C12HnCpprQu4"
      },
      "source": [
        "## Data utils\n",
        "\n",
        "Functions to download and process the experimental datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FKolGF1GAPma",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import requests\n",
        "from urllib import request \n",
        "from zipfile import ZipFile\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn.datasets\n",
        "from functools import reduce\n",
        "\n",
        "def normal(x,min_val=0):\n",
        "  '''\n",
        "  Normalize a vector\n",
        "\n",
        "  Parameters:\n",
        "  x = numerical vector\n",
        "  min_val = integer; choice of [0,1], setting whether normalization in range[0,1] or [-1,1]\n",
        "\n",
        "  Return:\n",
        "  x_norm = normalized vector\n",
        "  '''\n",
        "  x_min = np.min(x)\n",
        "  x_max = np.max(x)\n",
        "  if x_min == 0 and x_max == 0:\n",
        "    return x\n",
        "  if min_val == -1:\n",
        "    x_norm = 2 * ((x - x_min) / (x_max - x_min)) - 1\n",
        "  if min_val== 0:\n",
        "    x_norm = ((x - x_min) / (x_max - x_min))\n",
        "  return x_norm\n",
        "\n",
        "def get_election_data(pred=\"gop_2016\",norm_x=True,norm_y=True,norm_min_val=0,spat_int=True):\n",
        "  '''\n",
        "  Download and process the Election dataset used in CorrelationGNN (https://arxiv.org/abs/2002.08274)\n",
        "\n",
        "  Parameters:\n",
        "  pred = numeric; outcome variable to be returned; choose from [\"dem_2016\",\n",
        "                                                       \"gop_2016\",\n",
        "                                                       \"MedianIncome2016\",\n",
        "                                                       \"R_NET_MIG_2016\",\n",
        "                                                       \"R_birth_2016\",\n",
        "                                                       \"R_death_2016\",\n",
        "                                                       \"BachelorRate2016\",\n",
        "                                                       \"Unemployment_rate_2016\"]\n",
        "  norm_x = logical; should features be normalized\n",
        "  norm_y = logical; should outcome be normalized\n",
        "  norm_min_val = integer; choice of [0,1], setting whether normalization in range[0,1] or [-1,1]\n",
        "\n",
        "  Return:\n",
        "  coords = spatial coordinates (lon/lat)\n",
        "  x = features at location (excluding outcome variable)\n",
        "  y = outcome variable\n",
        "  '''\n",
        "\n",
        "  Path(\"./election_data\").mkdir(parents=True, exist_ok=True)\n",
        "  zipurl = 'https://www2.census.gov/geo/docs/maps-data/data/gazetteer/2020_Gazetteer/2020_Gaz_counties_national.zip'\n",
        "  with request.urlopen(zipurl) as zipresp:\n",
        "      with ZipFile(io.BytesIO(zipresp.read())) as zfile:\n",
        "          zfile.extractall('./election_data')\n",
        "\n",
        "  geo = pd.read_csv(\"./election_data/2020_Gaz_counties_national.txt\",sep='\\t')\n",
        "  geo = geo.rename(columns={\"GEOID\":\"FIPS\",'INTPTLONG                                                                                                               ':'INTPTLONG'})\n",
        "\n",
        "  url = 'https://raw.githubusercontent.com/000Justin000/gnn-residual-correlation/master/datasets/election/education.csv'\n",
        "  url_open = request.urlopen(url)\n",
        "  edu = pd.read_csv(io.StringIO(url_open.read().decode('utf-8'))) \n",
        "\n",
        "  url = 'https://raw.githubusercontent.com/000Justin000/gnn-residual-correlation/master/datasets/election/election.csv'\n",
        "  url_open = request.urlopen(url)\n",
        "  ele = pd.read_csv(io.StringIO(url_open.read().decode('utf-8'))) \n",
        "  ele = ele.rename(columns={\"fips_code\":\"FIPS\"})\n",
        "\n",
        "  url = 'https://raw.githubusercontent.com/000Justin000/gnn-residual-correlation/master/datasets/election/income.csv'\n",
        "  url_open = request.urlopen(url)\n",
        "  inc = pd.read_csv(io.StringIO(url_open.read().decode('utf-8'))) \n",
        "\n",
        "  url = 'https://raw.githubusercontent.com/000Justin000/gnn-residual-correlation/master/datasets/election/unemployment.csv'\n",
        "  url_open = request.urlopen(url)\n",
        "  une = pd.read_csv(io.StringIO(url_open.read().decode('utf-8'))) \n",
        "\n",
        "  url = 'https://raw.githubusercontent.com/000Justin000/gnn-residual-correlation/master/datasets/election/population.csv'\n",
        "  url_open = request.urlopen(url)\n",
        "  pop = pd.read_csv(io.StringIO(url_open.read().decode('utf-8'))) \n",
        "\n",
        "  dfs = [geo,edu,ele,inc,une,pop]\n",
        "  data = reduce(lambda  left,right: pd.merge(left,right,on=['FIPS'],how='outer'), dfs)\n",
        "  data = data.replace({',':''}, regex=True)\n",
        "\n",
        "  out_data = np.array([data.INTPTLONG,data.INTPTLAT,data.dem_2016,data.gop_2016,data.MedianIncome2016,data.R_NET_MIG_2016,data.R_birth_2016,data.R_death_2016,data.BachelorRate2016,data.Unemployment_rate_2016]).T.astype(float)\n",
        "  out_data = out_data[~np.isnan(out_data).any(axis=1)]\n",
        "  out_data = out_data[(out_data[:,0] > -130) & (out_data[:,0] < -50) & (out_data[:,1] > 22) & (out_data[:,1] < 50)]\n",
        "\n",
        "  coords = out_data[:,:2]\n",
        "  if pred == \"dem_2016\":\n",
        "    y = out_data[:,2]\n",
        "    x = out_data[:,3:]\n",
        "  if pred == \"gop_2016\":\n",
        "    y = out_data[:,3]\n",
        "    x = out_data[:,[2,4,5,6,7,8,9]]\n",
        "  if pred == \"MedianIncome2016\":\n",
        "    y = out_data[:,4]\n",
        "    x = out_data[:,[2,3,5,6,7,8,9]]\n",
        "  if pred == \"R_NET_MIG_2016\":\n",
        "    y = out_data[:,5]\n",
        "    x = out_data[:,[2,3,4,6,7,8,9]]\n",
        "  if pred == \"R_birth_2016\":\n",
        "    y = out_data[:,6]\n",
        "    x = out_data[:,[2,3,4,5,7,8,9]]\n",
        "  if pred == \"R_death_2016\":\n",
        "    y = out_data[:,7]\n",
        "    x = out_data[:,[2,3,4,5,6,8,9]]\n",
        "  if pred == \"BachelorRate2016\":\n",
        "    y = out_data[:,8]\n",
        "    x = out_data[:,[2,3,4,5,6,7,9]]\n",
        "  if pred == \"Unemployment_rate_2016\":\n",
        "    y = out_data[:,9]\n",
        "    x = out_data[:,[2,3,4,5,6,7,8]]\n",
        "\n",
        "  if norm_y==True:\n",
        "    y = normal(y,norm_min_val)\n",
        "  if norm_x==True:\n",
        "    for i in range(x.shape[1]):\n",
        "      x[:,i] = normal(x[:,i],norm_min_val)\n",
        "  if spat_int==True:\n",
        "      x = torch.ones(x.shape[0],1)\n",
        "  return torch.tensor(coords), torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "def get_cali_housing_data(norm_x=True,norm_y=True,norm_min_val=0,spat_int=False):\n",
        "  '''\n",
        "  Download and process the California Housing Dataset\n",
        "\n",
        "  Parameters:\n",
        "  norm_x = logical; should features be normalized\n",
        "  norm_y = logical; should outcome be normalized\n",
        "  norm_min_val = integer; choice of [0,1], setting whether normalization in range[0,1] or [-1,1]\n",
        "\n",
        "  Return:\n",
        "  coords = spatial coordinates (lon/lat)\n",
        "  x = features at location\n",
        "  y = outcome variable\n",
        "  '''\n",
        "  cali_housing_ds = sklearn.datasets.fetch_california_housing()\n",
        "  coords = np.array(cali_housing_ds.data[:,6:])\n",
        "  y = np.array(cali_housing_ds.target)\n",
        "  x = np.array(cali_housing_ds.data[:,:6])\n",
        "  if norm_y==True:\n",
        "    y = normal(y,norm_min_val)\n",
        "  if norm_x==True:\n",
        "    for i in range(x.shape[1]):\n",
        "      x[:,i] = normal(x[:,i],norm_min_val)\n",
        "  if spat_int==True:\n",
        "    x = torch.ones(x.shape[0],1)\n",
        "  return torch.tensor(coords), torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "def get_3d_road_data(norm_y=True,norm_min_val=0):\n",
        "  '''\n",
        "  Download and process the 3d road dataset\n",
        "\n",
        "  Parameters:\n",
        "  norm_x = logical; should features be normalized\n",
        "  norm_y = logical; should outcome be normalized\n",
        "  norm_min_val = integer; choice of [0,1], setting whether normalization in range[0,1] or [-1,1]\n",
        "\n",
        "  Return:\n",
        "  coords = spatial coordinates (lon/lat)\n",
        "  x = features at location; empty for this dataset\n",
        "  y = outcome variable\n",
        "  '''\n",
        "  # Both of the above sources contain the 3d road dataset\n",
        "  #url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00246/3D_spatial_network.txt\"\n",
        "  url=\"http://nrvis.com/data/mldata/3D_spatial_network.csv\"\n",
        "  s=requests.get(url).content\n",
        "  c=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
        "  c.columns = [\"id\",\"x\",\"y\",\"z\"]\n",
        "  coords = np.array(c[[\"x\",\"y\"]])\n",
        "  y = np.array(c[[\"z\"]])\n",
        "  if norm_y==True:\n",
        "    y = normal(y,norm_min_val)\n",
        "\n",
        "  return torch.tensor(coords), None, torch.tensor(y)\n",
        "\n",
        "def get_air_temp_data(pred=\"temp\",norm_y=True,norm_x=True,norm_min_val=0):\n",
        "  '''\n",
        "  Download and process the Global Air Temperature dataset\n",
        "\n",
        "  Parameters:\n",
        "  pred = numeric; outcome variable to be returned; choose from [\"temp\", \"prec\"]\n",
        "  norm_y = logical; should outcome be normalized\n",
        "  norm_min_val = integer; choice of [0,1], setting whether normalization in range[0,1] or [-1,1]\n",
        "\n",
        "  Return:\n",
        "  coords = spatial coordinates (lon/lat)\n",
        "  x = features at location\n",
        "  y = outcome variable\n",
        "  '''\n",
        "  url = 'https://springernature.figshare.com/ndownloader/files/12609182'\n",
        "  url_open = request.urlopen(url)\n",
        "  inc = np.array(pd.read_csv(io.StringIO(url_open.read().decode('utf-8'))))\n",
        "  coords = inc[:,:2]\n",
        "  if pred==\"temp\":\n",
        "    y = inc[:,4].reshape(-1)\n",
        "    x = inc[:,5]\n",
        "  else:\n",
        "    y = inc[:,5].reshape(-1)\n",
        "    x = inc[:,4]\n",
        "  if norm_y==True:\n",
        "    y = normal(y,norm_min_val)\n",
        "  if norm_x==True:\n",
        "    x = normal(x,norm_min_val).reshape(-1,1)\n",
        "\n",
        "  return torch.tensor(coords), torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, x, y, coords):\n",
        "      self.features = x\n",
        "      self.target = y\n",
        "      self.coords = coords\n",
        "    def __len__(self):\n",
        "      return len(self.features)\n",
        "    def __getitem__(self, idx):\n",
        "      return torch.tensor(self.features[idx]), torch.tensor(self.target[idx]), torch.tensor(self.coords[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PvkPQ7rBC62"
      },
      "source": [
        "## Spatial utils\n",
        "\n",
        "Functions for processing the geographic coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P4PdOMgAeUx8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from scipy.stats import wasserstein_distance\n",
        "from math import radians, cos, sin, asin, sqrt\n",
        "\n",
        "def deg_to_rad(x):\n",
        "  return x * math.pi / 180\n",
        "\n",
        "def latlon_to_cart(lat,lon):\n",
        "  x = np.cos(lat) * np.cos(lon)\n",
        "  y = np.cos(lat) * np.sin(lon)\n",
        "  z = np.sin(lat)\n",
        "  cart_coord = np.column_stack((x, y, z))\n",
        "  return cart_coord\n",
        "\n",
        "def haversine(lon1, lat1, lon2, lat2): \n",
        "    \"\"\"\n",
        "    Calculate the great circle distance between two points \n",
        "    on the earth (specified in decimal degrees)\n",
        "    \"\"\"\n",
        "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
        " \n",
        "    # haversine\n",
        "    dlon = lon2 - lon1 \n",
        "    dlat = lat2 - lat1 \n",
        "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
        "    c = 2 * asin(sqrt(a)) \n",
        "    r = 6371 \n",
        "    return c * r * 1000\n",
        "\n",
        "# Helper function for 2+d distance\n",
        "def newDistance(a, b, nd_dist=\"great_circle\"):  \n",
        "    # Distance options are [\"great_circle\" (2D only), \"euclidean\", \"wasserstein\" (for higher-dimensional coordinate embeddings)]\n",
        "    if a.shape[0]==2:\n",
        "      x1, y1 = a[0], a[1]\n",
        "      x2, y2 = b[0], b[1]\n",
        "      if nd_dist==\"euclidean\":\n",
        "        d = math.sqrt( ((x1-x2)**2)+((y1-y2)**2))\n",
        "      else:\n",
        "        d = haversine(x1,y1,x2,y2)\n",
        "    if a.shape[0]==3:\n",
        "      x1, y1, z1 = a[0], a[1], a[2]\n",
        "      x2, y2, z2 = b[0], b[1], b[2]\n",
        "      d = math.sqrt(math.pow(x2 - x1, 2) +\n",
        "                  math.pow(y2 - y1, 2) +\n",
        "                  math.pow(z2 - z1, 2)* 1.0) \n",
        "    if a.shape[0]>3:\n",
        "      if nd_dist==\"wasserstein\":\n",
        "        d = wasserstein_distance(a.reshape(-1).detach(),b.reshape(-1).detach())\n",
        "        #d = sgw_cpu(a.reshape(1,-1).detach(),b.reshape(1,-1).detach())\n",
        "      else:\n",
        "        d = torch.pow(a.reshape(1,1,-1) - b.reshape(1,1,-1), 2).sum(2) \n",
        "    return d \n",
        "\n",
        "# Helper function for edge weights\n",
        "def makeEdgeWeight(x, edge_index):\n",
        "  to = edge_index[0]\n",
        "  fro = edge_index[1]\n",
        "  edge_weight = []\n",
        "  for i in range(len(to)):\n",
        "    edge_weight.append(newDistance(x[to[i]],x[fro[i]])) # probably want to do inverse distance eventually\n",
        "  max_val = max(edge_weight)\n",
        "  rng = max_val - min(edge_weight)\n",
        "  edge_weight = [(max_val - elem) / rng for elem in edge_weight]\n",
        "  return torch.Tensor(edge_weight)\n",
        "\n",
        "# knn graph to adjacency matrix (probably already built)\n",
        "def knn_to_adj(knn, n):\n",
        "  adj_matrix = torch.zeros(n, n, dtype=float) #lil_matrix((n, n), dtype=float) \n",
        "  for i in range(len(knn[0])):\n",
        "    tow = knn[0][i]\n",
        "    fro = knn[1][i]\n",
        "    adj_matrix[tow,fro] = 1 # should be bidectional?\n",
        "  return adj_matrix.T\n",
        "\n",
        "def normal_torch(tensor,min_val=0):\n",
        "  t_min = torch.min(tensor)\n",
        "  t_max = torch.max(tensor)\n",
        "  if t_min == 0 and t_max == 0:\n",
        "    return torch.tensor(tensor)\n",
        "  if min_val == -1:\n",
        "    tensor_norm = 2 * ((tensor - t_min) / (t_max - t_min)) - 1\n",
        "  if min_val== 0:\n",
        "    tensor_norm = ((tensor - t_min) / (t_max - t_min))\n",
        "  return torch.tensor(tensor_norm)\n",
        "\n",
        "def lw_tensor_local_moran(y,w_sparse,na_to_zero=True,norm=True,norm_min_val=0):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  y = y.reshape(-1)\n",
        "  n = len(y)\n",
        "  n_1 = n - 1\n",
        "  z = y - y.mean()\n",
        "  sy = y.std()\n",
        "  z /= sy\n",
        "  den = (z * z).sum()\n",
        "  zl = torch.tensor(w_sparse * z).to(device)\n",
        "  mi = n_1 * z * zl / den\n",
        "  if na_to_zero==True:\n",
        "    mi[torch.isnan(mi)] = 0\n",
        "  if norm==True:\n",
        "    mi = normal_torch(mi,min_val=norm_min_val)\n",
        "  return torch.tensor(mi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J73ImOl3rlIm"
      },
      "source": [
        "## Model\n",
        "\n",
        "Here are the different modules needed for PE-GNN. The Positional Encoder (PE) modules are adapted building on the original [Space2Vec](https://openreview.net/forum?id=rJljdh4KDH) code: https://github.com/gengchenmai/space2vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BuyL2TsJhZva",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.utils.data\n",
        "import math\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    layer normalization\n",
        "    Simple layer norm object optionally used with the convolutional encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones((feature_dim,)))\n",
        "        self.register_parameter(\"gamma\", self.gamma)\n",
        "        self.beta = nn.Parameter(torch.zeros((feature_dim,)))\n",
        "        self.register_parameter(\"beta\", self.beta)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, embed_dim]\n",
        "        # normalize for each embedding\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        # output shape is the same as x\n",
        "        # Type not match for self.gamma and self.beta??????????????????????\n",
        "        # output: [batch_size, embed_dim]\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "def get_activation_function(activation, context_str):\n",
        "    if activation == \"leakyrelu\":\n",
        "        return nn.LeakyReLU(negative_slope=0.2)\n",
        "    elif activation == \"relu\":\n",
        "        return nn.ReLU()\n",
        "    elif activation == \"sigmoid\":\n",
        "        return nn.Sigmoid()\n",
        "    elif activation == 'tanh':\n",
        "        return nn.Tanh()\n",
        "    else:\n",
        "        raise Exception(\"{} activation not recognized.\".format(context_str))\n",
        "\n",
        "\n",
        "class SingleFeedForwardNN(nn.Module):\n",
        "    \"\"\"\n",
        "        Creates a single layer fully connected feed forward neural network.\n",
        "        this will use non-linearity, layer normalization, dropout\n",
        "        this is for the hidden layer, not the last layer of the feed forard NN\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim,\n",
        "                    output_dim,\n",
        "                    dropout_rate=None,\n",
        "                    activation=\"sigmoid\",\n",
        "                    use_layernormalize=False,\n",
        "                    skip_connection = False,\n",
        "                    context_str = ''):\n",
        "        '''\n",
        "        Args:\n",
        "            input_dim (int32): the input embedding dim\n",
        "            output_dim (int32): dimension of the output of the network.\n",
        "            dropout_rate (scalar tensor or float): Dropout keep prob.\n",
        "            activation (string): tanh or relu or leakyrelu or sigmoid\n",
        "            use_layernormalize (bool): do layer normalization or not\n",
        "            skip_connection (bool): do skip connection or not\n",
        "            context_str (string): indicate which spatial relation encoder is using the current FFN\n",
        "        '''\n",
        "        super(SingleFeedForwardNN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if dropout_rate is not None:\n",
        "            self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        else:\n",
        "            self.dropout = None\n",
        "\n",
        "        self.act = get_activation_function(activation, context_str)\n",
        "\n",
        "        if use_layernormalize:\n",
        "            # the layer normalization is only used in the hidden layer, not the last layer\n",
        "            self.layernorm = nn.LayerNorm(self.output_dim)\n",
        "        else:\n",
        "            self.layernorm = None\n",
        "\n",
        "        # the skip connection is only possible, if the input and out dimention is the same\n",
        "        if self.input_dim == self.output_dim:\n",
        "            self.skip_connection = skip_connection\n",
        "        else:\n",
        "            self.skip_connection = False\n",
        "        \n",
        "        self.linear = nn.Linear(self.input_dim, self.output_dim)\n",
        "        nn.init.xavier_uniform(self.linear.weight)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        '''\n",
        "        Args:\n",
        "            input_tensor: shape [batch_size, ..., input_dim]\n",
        "        Returns:\n",
        "            tensor of shape [batch_size,..., output_dim]\n",
        "            note there is no non-linearity applied to the output.\n",
        "        Raises:\n",
        "            Exception: If given activation or normalizer not supported.\n",
        "        '''\n",
        "        assert input_tensor.size()[-1] == self.input_dim\n",
        "        # Linear layer\n",
        "        output = self.linear(input_tensor)\n",
        "        # non-linearity\n",
        "        output = self.act(output)\n",
        "        # dropout\n",
        "        if self.dropout is not None:\n",
        "            output = self.dropout(output)\n",
        "\n",
        "        # skip connection\n",
        "        if self.skip_connection:\n",
        "            output = output + input_tensor\n",
        "\n",
        "        # layer normalization\n",
        "        if self.layernorm is not None:\n",
        "            output = self.layernorm(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class MultiLayerFeedForwardNN(nn.Module):\n",
        "    \"\"\"\n",
        "        Creates a fully connected feed forward neural network.\n",
        "        N fully connected feed forward NN, each hidden layer will use non-linearity, layer normalization, dropout\n",
        "        The last layer do not have any of these\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim,\n",
        "                    output_dim,\n",
        "                    num_hidden_layers=0,\n",
        "                    dropout_rate=0.5,\n",
        "                    hidden_dim=-1,\n",
        "                    activation=\"relu\",\n",
        "                    use_layernormalize=True,\n",
        "                    skip_connection = False,\n",
        "                    context_str = None):\n",
        "        '''\n",
        "        Args:\n",
        "            input_dim (int32): the input embedding dim\n",
        "            num_hidden_layers (int32): number of hidden layers in the network, set to 0 for a linear network.\n",
        "            output_dim (int32): dimension of the output of the network.\n",
        "            dropout (scalar tensor or float): Dropout keep prob.\n",
        "            hidden_dim (int32): size of the hidden layers\n",
        "            activation (string): tanh or relu\n",
        "            use_layernormalize (bool): do layer normalization or not\n",
        "            context_str (string): indicate which spatial relation encoder is using the current FFN\n",
        "        '''\n",
        "        super(MultiLayerFeedForwardNN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.activation = activation\n",
        "        self.use_layernormalize = use_layernormalize\n",
        "        self.skip_connection = skip_connection\n",
        "        self.context_str = context_str\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        if self.num_hidden_layers <= 0:\n",
        "            self.layers.append( SingleFeedForwardNN(input_dim = self.input_dim,\n",
        "                                                    output_dim = self.output_dim,\n",
        "                                                    dropout_rate = self.dropout_rate,\n",
        "                                                    activation = self.activation,\n",
        "                                                    use_layernormalize = False,\n",
        "                                                    skip_connection = False,\n",
        "                                                    context_str = self.context_str))\n",
        "        else:\n",
        "            self.layers.append( SingleFeedForwardNN(input_dim = self.input_dim,\n",
        "                                                    output_dim = self.hidden_dim,\n",
        "                                                    dropout_rate = self.dropout_rate,\n",
        "                                                    activation = self.activation,\n",
        "                                                    use_layernormalize = self.use_layernormalize,\n",
        "                                                    skip_connection = self.skip_connection,\n",
        "                                                    context_str = self.context_str))\n",
        "\n",
        "            for i in range(self.num_hidden_layers-1):\n",
        "                self.layers.append( SingleFeedForwardNN(input_dim = self.hidden_dim,\n",
        "                                                    output_dim = self.hidden_dim,\n",
        "                                                    dropout_rate = self.dropout_rate,\n",
        "                                                    activation = self.activation,\n",
        "                                                    use_layernormalize = self.use_layernormalize,\n",
        "                                                    skip_connection = self.skip_connection,\n",
        "                                                    context_str = self.context_str))\n",
        "\n",
        "            self.layers.append( SingleFeedForwardNN(input_dim = self.hidden_dim,\n",
        "                                                    output_dim = self.output_dim,\n",
        "                                                    dropout_rate = self.dropout_rate,\n",
        "                                                    activation = self.activation,\n",
        "                                                    use_layernormalize = False,\n",
        "                                                    skip_connection = False,\n",
        "                                                    context_str = self.context_str))\n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        '''\n",
        "        Args:\n",
        "            input_tensor: shape [batch_size, ..., input_dim]\n",
        "        Returns:\n",
        "            tensor of shape [batch_size, ..., output_dim]\n",
        "            note there is no non-linearity applied to the output.\n",
        "        Raises:\n",
        "            Exception: If given activation or normalizer not supported.\n",
        "        '''\n",
        "        assert input_tensor.size()[-1] == self.input_dim\n",
        "        output = input_tensor\n",
        "        for i in range(len(self.layers)):\n",
        "            output = self.layers[i](output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def _cal_freq_list(freq_init, frequency_num, max_radius, min_radius):\n",
        "    if freq_init == \"random\":\n",
        "        freq_list = np.random.random(size=[frequency_num]) * max_radius\n",
        "    elif freq_init == \"geometric\":\n",
        "        log_timescale_increment = (math.log(float(max_radius) / float(min_radius)) / (frequency_num*1.0 - 1))\n",
        "        timescales = min_radius * np.exp(np.arange(frequency_num).astype(float) * log_timescale_increment)\n",
        "        freq_list = 1.0/timescales\n",
        "    return freq_list\n",
        "\n",
        "class GridCellSpatialRelationEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Given a list of (deltaX,deltaY), encode them using the position encoding function\n",
        "    \"\"\"\n",
        "    def __init__(self, spa_embed_dim, coord_dim = 2, frequency_num = 16, \n",
        "            max_radius =0.01, min_radius = 0.00001,\n",
        "            freq_init = \"geometric\",\n",
        "            ffn=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            spa_embed_dim: the output spatial relation embedding dimention\n",
        "            coord_dim: the dimention of space, 2D, 3D, or other\n",
        "            frequency_num: the number of different sinusoidal with different frequencies/wavelengths\n",
        "            max_radius: the largest context radius this model can handle\n",
        "        \"\"\"\n",
        "        super(GridCellSpatialRelationEncoder, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.spa_embed_dim = spa_embed_dim\n",
        "        self.coord_dim = coord_dim \n",
        "        self.frequency_num = frequency_num\n",
        "        self.freq_init = freq_init\n",
        "        self.max_radius = max_radius\n",
        "        self.min_radius = min_radius\n",
        "        self.ffn = ffn\n",
        "        # the frequence we use for each block, alpha in ICLR paper\n",
        "        self.cal_freq_list()\n",
        "        self.cal_freq_mat()\n",
        "        self.input_embed_dim = self.cal_input_dim()\n",
        "\n",
        "        if self.ffn is not None:\n",
        "          self.ffn = MultiLayerFeedForwardNN(2 * frequency_num * 2, spa_embed_dim)\n",
        "\n",
        "    def cal_elementwise_angle(self, coord, cur_freq):\n",
        "        '''\n",
        "        Args:\n",
        "            coord: the deltaX or deltaY\n",
        "            cur_freq: the frequency\n",
        "        '''\n",
        "        return coord/(np.power(self.max_radius, cur_freq*1.0/(self.frequency_num-1)))\n",
        "\n",
        "    def cal_coord_embed(self, coords_tuple):\n",
        "        embed = []\n",
        "        for coord in coords_tuple:\n",
        "            for cur_freq in range(self.frequency_num):\n",
        "                embed.append(math.sin(self.cal_elementwise_angle(coord, cur_freq)))\n",
        "                embed.append(math.cos(self.cal_elementwise_angle(coord, cur_freq)))\n",
        "        # embed: shape (input_embed_dim)\n",
        "        return embed\n",
        "\n",
        "    def cal_input_dim(self):\n",
        "        # compute the dimention of the encoded spatial relation embedding\n",
        "        return int(self.coord_dim * self.frequency_num * 2)\n",
        "\n",
        "    def cal_freq_list(self):\n",
        "        self.freq_list = _cal_freq_list(self.freq_init, self.frequency_num, self.max_radius, self.min_radius)\n",
        "\n",
        "\n",
        "    def cal_freq_mat(self):\n",
        "        # freq_mat shape: (frequency_num, 1)\n",
        "        freq_mat = np.expand_dims(self.freq_list, axis = 1)\n",
        "        # self.freq_mat shape: (frequency_num, 2)\n",
        "        self.freq_mat = np.repeat(freq_mat, 2, axis = 1)\n",
        "\n",
        "    def make_input_embeds(self, coords):\n",
        "        if type(coords) == np.ndarray:\n",
        "            assert self.coord_dim == np.shape(coords)[2]\n",
        "            coords = list(coords)\n",
        "        elif type(coords) == list:\n",
        "            assert self.coord_dim == len(coords[0][0])\n",
        "        else:\n",
        "            raise Exception(\"Unknown coords data type for GridCellSpatialRelationEncoder\")\n",
        "        \n",
        "        # coords_mat: shape (batch_size, num_context_pt, 2)\n",
        "        coords_mat = np.asarray(coords).astype(float)\n",
        "        batch_size = coords_mat.shape[0]\n",
        "        num_context_pt = coords_mat.shape[1]\n",
        "        # coords_mat: shape (batch_size, num_context_pt, 2, 1)\n",
        "        coords_mat = np.expand_dims(coords_mat, axis = 3)\n",
        "        # coords_mat: shape (batch_size, num_context_pt, 2, 1, 1)\n",
        "        coords_mat = np.expand_dims(coords_mat, axis = 4)\n",
        "        # coords_mat: shape (batch_size, num_context_pt, 2, frequency_num, 1)\n",
        "        coords_mat = np.repeat(coords_mat, self.frequency_num, axis = 3)\n",
        "        # coords_mat: shape (batch_size, num_context_pt, 2, frequency_num, 2)\n",
        "        coords_mat = np.repeat(coords_mat, 2, axis = 4)\n",
        "        # spr_embeds: shape (batch_size, num_context_pt, 2, frequency_num, 2)\n",
        "        spr_embeds = coords_mat * self.freq_mat\n",
        "        # make sinuniod function\n",
        "        # sin for 2i, cos for 2i+1\n",
        "        # spr_embeds: (batch_size, num_context_pt, 2*frequency_num*2=input_embed_dim)\n",
        "        spr_embeds[:, :, :, :, 0::2] = np.sin(spr_embeds[:, :, :, :, 0::2])  # dim 2i\n",
        "        spr_embeds[:, :, :, :, 1::2] = np.cos(spr_embeds[:, :, :, :, 1::2])  # dim 2i+1\n",
        "        # (batch_size, num_context_pt, 2*frequency_num*2)\n",
        "        spr_embeds = np.reshape(spr_embeds, (batch_size, num_context_pt, -1))\n",
        "        return spr_embeds\n",
        "\n",
        "    def forward(self, coords):\n",
        "        \"\"\"\n",
        "        Given a list of coords (deltaX, deltaY), give their spatial relation embedding\n",
        "        Args:\n",
        "            coords: a python list with shape (batch_size, num_context_pt, coord_dim)\n",
        "        Return:\n",
        "            sprenc: Tensor shape (batch_size, num_context_pt, spa_embed_dim)\n",
        "        \"\"\"   \n",
        "        spr_embeds = self.make_input_embeds(coords)\n",
        "        spr_embeds = torch.FloatTensor(spr_embeds).to(self.device)\n",
        "        if self.ffn is not None:\n",
        "            return self.ffn(spr_embeds)\n",
        "        else:\n",
        "            return spr_embeds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4owDoCqhczv"
      },
      "source": [
        "The GNN backbones are defined here, with a GCN as example (though other layers like GAT or GraphSAGE can be used interchangeably with the GCN layers), using *PyTorch Geometric*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oKM6lwrUAp-X",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class GCN(nn.Module):\n",
        "    \"\"\"\n",
        "        GCN\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features_in=3, num_features_out=1, k=20, MAT=False):\n",
        "        super(GCN, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.k = k\n",
        "        self.MAT = MAT\n",
        "        self.conv1 = GCNConv(num_features_in, 32)\n",
        "        self.conv2 = GCNConv(32, 32)\n",
        "        self.fc = nn.Linear(32, num_features_out)\n",
        "        if MAT:\n",
        "          self.fc_morans = nn.Linear(32, num_features_out)\n",
        "    def forward(self, x, c, ei, ew):\n",
        "        x = x.float()\n",
        "        c = c.float()\n",
        "        if torch.is_tensor(ei) & torch.is_tensor(ew):\n",
        "          edge_index = ei\n",
        "          edge_weight = ew\n",
        "        else:\n",
        "          edge_index = knn_graph(c, k=self.k).to(self.device)\n",
        "          edge_weight = makeEdgeWeight(c, edge_index).to(self.device)\n",
        "        h1 = F.relu(self.conv1(x, edge_index, edge_weight))\n",
        "        h1 = F.dropout(h1, training=self.training)\n",
        "        h2 = F.relu(self.conv2(h1, edge_index, edge_weight))\n",
        "        h2 = F.dropout(h2, training=self.training)\n",
        "        output = self.fc(h2)\n",
        "        if self.MAT:\n",
        "          morans_output = self.fc_morans(h2)\n",
        "          return output, morans_output\n",
        "        else:\n",
        "          return output\n",
        "\n",
        "class PEGCN(nn.Module):\n",
        "    \"\"\"\n",
        "        GCN with positional encoder and auxiliary tasks\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features_in=3, num_features_out=1, emb_hidden_dim=128, emb_dim=16, k = 20, MAT=False):\n",
        "        super(PEGCN, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.emb_hidden_dim = emb_hidden_dim\n",
        "        self.emb_dim = emb_dim\n",
        "        self.k = k\n",
        "        self.MAT = MAT\n",
        "        self.spenc = GridCellSpatialRelationEncoder(spa_embed_dim=emb_hidden_dim,ffn=True,min_radius=1e-06,max_radius=360)\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.Linear(emb_hidden_dim, emb_hidden_dim // 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(emb_hidden_dim // 2, emb_hidden_dim // 4),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(emb_hidden_dim // 4, emb_dim)\n",
        "        )\n",
        "        self.conv1 = GCNConv(num_features_in + emb_dim, 32)\n",
        "        self.conv2 = GCNConv(32, 32)\n",
        "        self.fc = nn.Linear(32, num_features_out)\n",
        "        if MAT:\n",
        "          self.fc_morans = nn.Linear(32, num_features_out)\n",
        "    def forward(self, x, c, ei, ew):\n",
        "        x = x.float()\n",
        "        c = c.float()\n",
        "        if torch.is_tensor(ei) & torch.is_tensor(ew):\n",
        "          edge_index = ei\n",
        "          edge_weight = ew\n",
        "        else:\n",
        "          edge_index = knn_graph(c, k=self.k).to(self.device)\n",
        "          edge_weight = makeEdgeWeight(c, edge_index).to(self.device)\n",
        "\n",
        "        c = c.reshape(1, c.shape[0], c.shape[1])\n",
        "        emb = self.spenc(c.detach().cpu().numpy())\n",
        "        emb = emb.reshape(emb.shape[1],emb.shape[2])\n",
        "        emb = self.dec(emb).float()\n",
        "        x = torch.cat((x,emb),dim=1)\n",
        "\n",
        "        h1 = F.relu(self.conv1(x, edge_index, edge_weight))\n",
        "        h1 = F.dropout(h1, training=self.training)\n",
        "        h2 = F.relu(self.conv2(h1, edge_index, edge_weight))\n",
        "        h2 = F.dropout(h2, training=self.training)\n",
        "        output = self.fc(h2)\n",
        "        if self.MAT:\n",
        "          morans_output = self.fc_morans(h2)\n",
        "          return output, morans_output\n",
        "        else:\n",
        "          return output\n",
        "\n",
        "class LossWrapper(nn.Module):\n",
        "    def __init__(self, model, task_num=1, loss='mse', uw=True, lamb=0.5, k=20, batch_size=2048):\n",
        "        super(LossWrapper, self).__init__()\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = model\n",
        "        self.task_num = task_num\n",
        "        self.uw = uw\n",
        "        self.lamb = lamb\n",
        "        self.k = k\n",
        "        self.batch_size = batch_size\n",
        "        if task_num > 1:\n",
        "          self.log_vars = nn.Parameter(torch.zeros((task_num)))\n",
        "        if loss==\"mse\":\n",
        "          self.criterion = nn.MSELoss()\n",
        "        elif loss==\"l1\":\n",
        "          self.criterion = nn.L1Loss()\n",
        "\n",
        "    def forward(self, input, targets, coords, edge_index, edge_weight, morans_input):\n",
        "\n",
        "        if self.task_num==1:\n",
        "          outputs = self.model(input, coords, edge_index, edge_weight)\n",
        "          loss = self.criterion(targets.float().reshape(-1),outputs.float().reshape(-1))\n",
        "          return loss\n",
        "\n",
        "        else:\n",
        "          outputs1, outputs2 = self.model(input, coords, edge_index, edge_weight)\n",
        "          if torch.is_tensor(morans_input):\n",
        "            targets2 = morans_input\n",
        "          else:\n",
        "            moran_weight_matrix = knn_to_adj(knn_graph(coords, k=self.k), self.batch_size) \n",
        "            with torch.enable_grad():\n",
        "              targets2 = lw_tensor_local_moran(targets, sparse.csr_matrix(moran_weight_matrix)).to(self.device)\n",
        "          if self.uw:\n",
        "            precision1 = 0.5 * torch.exp(-self.log_vars[0])\n",
        "            loss1 = self.criterion(targets.float().reshape(-1),outputs1.float().reshape(-1))\n",
        "            loss1 = torch.sum(precision1 * loss1 + self.log_vars[0], -1)\n",
        "\n",
        "            precision2 = 0.5 * torch.exp(-self.log_vars[1])\n",
        "            loss2 = self.criterion(targets2.float().reshape(-1),outputs2.float().reshape(-1))\n",
        "            loss2 = torch.sum(precision2 * loss2 + self.log_vars[1], -1)\n",
        "\n",
        "            loss = loss1 + loss2\n",
        "            loss = torch.mean(loss)\n",
        "            return loss, self.log_vars.data.tolist()\n",
        "          else:\n",
        "            loss1 = self.criterion(targets.float().reshape(-1),outputs1.float().reshape(-1))\n",
        "            loss2 = self.criterion(targets2.float().reshape(-1),outputs2.float().reshape(-1))\n",
        "            loss = loss1 + self.lamb * loss2\n",
        "            return loss        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIMlNtW4T0T4"
      },
      "source": [
        "## Training\n",
        "\n",
        "The final training loop for *PE-GNN*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sAEtAN7kTzoi",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def train(args):\n",
        "  # Get args\n",
        "  dset = args.dset\n",
        "  model_name = args.model_name\n",
        "  random_state = args.random_state\n",
        "  path = args.path\n",
        "  train_size = args.train_size\n",
        "  batched_training = args.batched_training\n",
        "  batch_size = args.batch_size\n",
        "  n_epochs = args.n_epochs\n",
        "  train_crit = args.train_crit\n",
        "  lr = args.lr\n",
        "  emb_dim = args.emb_dim\n",
        "  MAT = args.mat\n",
        "  uw = args.uw\n",
        "  lamb = args.lamb\n",
        "  k = args.k\n",
        "  save_freq = args.save_freq\n",
        "  print_progress = args.print_progress\n",
        "\n",
        "  # Set random seed\n",
        "  np.random.seed(random_state)\n",
        "\n",
        "  # Set device\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  # Access and process data\n",
        "  if dset==\"cali_housing\":\n",
        "    c,x,y = get_cali_housing_data()\n",
        "  if dset==\"election\":\n",
        "    c,x,y = get_election_data()\n",
        "  if dset==\"air_temp\":\n",
        "    c,x,y = get_air_temp_data()\n",
        "  if dset==\"3d_road\":\n",
        "    c,x,y = get_3d_road_data()\n",
        "    x = torch.ones(y.shape[0],1) #Dummies\n",
        "\n",
        "  n = x.shape[0]\n",
        "  n_train = np.round(n * train_size).astype(int)\n",
        "  n_test = (n - n_train).astype(int)\n",
        "  indices = np.arange(n)\n",
        "  _, _, _, _, idx_train, idx_test = train_test_split(x, y, indices, test_size=(1-train_size), random_state=random_state)\n",
        "\n",
        "  train_x, test_x = x[idx_train], x[idx_test]\n",
        "  train_y, test_y = y[idx_train], y[idx_test]\n",
        "  train_c, test_c = c[idx_train], c[idx_test]\n",
        "  train_dataset, test_dataset = MyDataset(train_x, train_y, train_c), MyDataset(test_x, test_y, test_c)\n",
        "  \n",
        "  if batched_training==False:\n",
        "    batch_size = len(idx_train)\n",
        "    train_edge_index = knn_graph(train_c, k=k).to(device)\n",
        "    train_edge_weight = makeEdgeWeight(train_c, train_edge_index).to(device)\n",
        "    test_edge_index = knn_graph(test_c, k=k).to(device)\n",
        "    test_edge_weight = makeEdgeWeight(test_c, test_edge_index).to(device)\n",
        "    train_moran_weight_matrix = knn_to_adj(train_edge_index, batch_size) #libpysal.weights.KNN(batch_y.cpu(), k=20).to(device)\n",
        "    with torch.enable_grad():\n",
        "      train_y_moran = lw_tensor_local_moran(train_y, sparse.csr_matrix(train_moran_weight_matrix)).to(device)\n",
        "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False, drop_last=False)\n",
        "  else:\n",
        "    train_edge_index = False\n",
        "    train_edge_weight = False\n",
        "    test_edge_index = False\n",
        "    test_edge_weight = False\n",
        "    train_y_moran = False\n",
        "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "  # Make model\n",
        "  if model_name==\"gcn\":\n",
        "    model = GCN(num_features_in=train_x.shape[1],k=k,MAT=MAT).to(device)\n",
        "  if model_name==\"pegcn\":\n",
        "    model = PEGCN(num_features_in=train_x.shape[1],k=k,MAT=MAT,emb_dim=emb_dim).to(device)\n",
        "  model = model.float()\n",
        "\n",
        "  if MAT:\n",
        "    task_num = 2\n",
        "  else:\n",
        "    task_num = 1\n",
        "\n",
        "  loss_wrapper = LossWrapper(model, task_num=task_num, loss=train_crit, uw=uw, lamb=lamb, k=k, batch_size=batch_size).to(device)\n",
        "  optimizer = torch.optim.Adam(loss_wrapper.parameters(), lr=lr)\n",
        "  score1 = nn.MSELoss()\n",
        "  score2 = nn.L1Loss()\n",
        "  \n",
        "  # Tensorboard and logging\n",
        "  test_ = dset + '-' + model_name + '-k' + str(k)\n",
        "  if model_name=='pegcn':\n",
        "    test_ = test_ + '-emb' + str(emb_dim)\n",
        "  if MAT:\n",
        "    if uw:\n",
        "      test_ = test_ + \"_mat-uw\"\n",
        "    else:\n",
        "      test_ = test_ + \"_mat-lam\" + str(lamb)\n",
        "  if batched_training==True:\n",
        "    test_ = test_ + \"_bs\" + str(batch_size) + \"_ep\" + str(n_epochs)\n",
        "  else:\n",
        "    test_ = test_ + \"_bsn_ep\" + str(n_epochs)\n",
        "\n",
        "  saved_file = \"{}_{}{}-{}:{}:{}.{}\".format(test_,\n",
        "                                            datetime.now().strftime(\"%h\"),\n",
        "                                            datetime.now().strftime(\"%d\"),\n",
        "                                            datetime.now().strftime(\"%H\"),\n",
        "                                            datetime.now().strftime(\"%M\"),\n",
        "                                            datetime.now().strftime(\"%S\"),\n",
        "                                            datetime.now().strftime(\"%f\"))\n",
        "\n",
        "  log_dir = path + \"/trained/{}/log\".format(saved_file)\n",
        "\n",
        "  if not os.path.exists(path + \"trained/{}/data\".format(saved_file)):\n",
        "      os.makedirs(path + \"/trained/{}/data\".format(saved_file))\n",
        "  if not os.path.exists(path + \"/trained/{}/images\".format(saved_file)):\n",
        "      os.makedirs(path + \"/trained/{}/images\".format(saved_file))\n",
        "  with open(path + \"/trained/{}/train_notes.txt\".format(saved_file), 'w') as f:\n",
        "      # Include any experiment notes here:\n",
        "      f.write(\"Experiment notes: .... \\n\\n\")\n",
        "      f.write(\"MODEL_DATA: {}\\n\".format(\n",
        "          test_))\n",
        "      f.write(\"BATCH_SIZE: {}\\nLEARNING_RATE: {}\\n\".format(\n",
        "          batch_size,\n",
        "          lr))\n",
        "  \n",
        "  writer = SummaryWriter(log_dir)\n",
        "  \n",
        "  # Training loop\n",
        "  it_counts = 0\n",
        "  for epoch in range(n_epochs):\n",
        "    for batch in train_loader:\n",
        "      model.train()\n",
        "      it_counts += 1\n",
        "      x = batch[0].to(device).float()\n",
        "      y = batch[1].to(device).float()\n",
        "      c = batch[2].to(device).float()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if MAT==True & uw==True:\n",
        "        loss, log_vars = loss_wrapper(x, y, c, train_edge_index, train_edge_weight, train_y_moran)\n",
        "      else:\n",
        "        loss = loss_wrapper(x, y, c, train_edge_index, train_edge_weight, train_y_moran)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      # Eval \n",
        "      if it_counts % save_freq == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          if MAT:\n",
        "            pred,_ = model(torch.tensor(test_dataset.features).to(device),torch.tensor(test_dataset.coords).to(device),test_edge_index,test_edge_weight)\n",
        "          else:\n",
        "            pred = model(torch.tensor(test_dataset.features).to(device),torch.tensor(test_dataset.coords).to(device),test_edge_index,test_edge_weight)\n",
        "        test_score1 = score1(torch.tensor(test_dataset.target).reshape(-1).to(device), pred.reshape(-1))\n",
        "        test_score2 = score2(torch.tensor(test_dataset.target).reshape(-1).to(device), pred.reshape(-1))\n",
        "\n",
        "        if print_progress:\n",
        "          print(\"Epoch [%d/%d] - Loss: %f - Test score (MSE): %f - Test score (MAE): %f\" % (epoch, n_epochs, loss.item(), test_score1.item(), test_score2.item()))\n",
        "        save_path = path + \"/trained/{}/ckpts\".format(saved_file)\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "        torch.save(model, save_path + '/' + 'model.pt')\n",
        "        writer.add_scalar('Test score (MSE)', test_score1.item(), it_counts)\n",
        "        writer.add_scalar('Test score (MAE)', test_score2.item(), it_counts)\n",
        "      writer.add_scalar('Training loss', loss.item(), it_counts)\n",
        "      if MAT==True & uw==True:\n",
        "        writer.add_scalar('Uncertainty weight: main task', log_vars[0], it_counts)\n",
        "        writer.add_scalar('Uncertainty weight: Morans aux task', log_vars[1], it_counts)  \n",
        "      writer.flush()\n",
        "  print(\"Saved all models to {}\".format(save_path))\n",
        "  print(\"Epoch [%d/%d] - Loss: %f - Test score (MSE): %f - Test score (MAE): %f\" % (epoch, n_epochs, loss.item(), test_score1.item(), test_score2.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM78AOniaVl1"
      },
      "source": [
        "Load tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLgQ6I01CzEw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80E2uOoRaXSc"
      },
      "source": [
        "## Training \n",
        "\n",
        "Set your parameters and train away!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ1MH2Rtp1J7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "  \n",
        "  parser = argparse.ArgumentParser(description='PE-GCN')\n",
        "  # Data & model selection\n",
        "  parser.add_argument('-d', '--dset', type=str, default='air_temp',\n",
        "                      choices=['cali_housing', 'election', 'air_temp', '3d_road'])\n",
        "  parser.add_argument('-m', '--model_name',  type=str, default='gcn', choices=['gcn','pegcn'])\n",
        "  # Utilities\n",
        "  parser.add_argument('-s', '--random_state', type=int, default=1)\n",
        "  parser.add_argument('-p', '--path', type=str, default='./')\n",
        "  # Training setting \n",
        "  parser.add_argument('-ts', '--train_size', type=float, default=0.8)\n",
        "  parser.add_argument('-bt', '--batched_training', type=bool, default=True)\n",
        "  parser.add_argument('-bs', '--batch_size', type=int, default=1024)\n",
        "  parser.add_argument('-ne', '--n_epochs', type=int, default=50)\n",
        "  parser.add_argument('-loss', '--train_crit', type=str, default='mse', choices=['mse','l1'])\n",
        "  parser.add_argument('-lr', '--lr', type=float, default=1e-3)\n",
        "  # Model config\n",
        "  parser.add_argument('-embd', '--emb_dim', type=float, default=64)\n",
        "  parser.add_argument('-mat', '--mat', type=bool, default=False)\n",
        "  parser.add_argument('-u', '--uw', type=bool, default=False)\n",
        "  parser.add_argument('-l', '--lamb', type=float, default=0.5)\n",
        "  parser.add_argument('-k', '--k', type=int, default=5)\n",
        "  # Logging & evaluation\n",
        "  parser.add_argument('-save', '--save_freq', type=int, default=5)\n",
        "  parser.add_argument('-print', '--print_progress', type=bool, default=True)\n",
        "  parser.add_argument('-f') #Dummy to get parser to run in Colab\n",
        "\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  out = train(args)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PE_GNN_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
